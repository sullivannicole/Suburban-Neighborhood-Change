---
title: "Longitudinal Clustering"
output: html_notebook
---

----------------------------------------

Longitudinal Clustering, 3-Timepoints; Random Forest imputation; incl. % new builds

----------------------------------------

```{r}
library(tidyverse)
library(kml3d) # for longitudinal clustering
library(extrafont) # for fonts for plots
library(pander) # to tidy model output
library(openxlsx)
library(data.table)
library(R.utils) # to unzip gz compressed file
library(missForest) # random forest imputation of missing rent, and HHI
```

## Import 3-timepoint data

```{r}
# Import data - enriched ACS created by Matt and copied to this folder from CommDev/Research/Research/Census Data/ACS/Excel Data
setwd("Data/Tract-level")
acs <- list.files(pattern = "acs2.*.xlsx")

list2env(purrr::map(setNames(acs, make.names(gsub("*5_tr.xlsx$", "", acs))), 
         read.xlsx), envir = .GlobalEnv)

# Select variables

acs_df <- function(df) {
  
year <- df$YEAR %>% unique()

acs <<- df %>%
  dplyr::select(GEOID2, TCFLAG, AVGHHSIZE, WHITENH, POPTOTAL, AGEUNDER18, AGE65UP, BACHELORS, GRADPROF, POPOVER25, HH_NOVEH, HHTOTAL, LEP, HHMOBILE, OWNEROCC, RENTEROCC, ASIANNH, BLACKNH, HISPPOP, AMINDNH, PACIFICNH, OTHERNH, MULTRACENH, POV185RATE, MEDGRENT, MEDHOMEVAL, MEDIANHHI) %>%
  filter(TCFLAG == 1) %>% # Select metro tracts (includes core and MUSA)
  dplyr::select(-TCFLAG) %>%
  mutate(PC_POC = (POPTOTAL-WHITENH)/POPTOTAL*100,
         PC_18UNDER = AGEUNDER18/POPTOTAL*100,
         PC_65UP = AGE65UP/POPTOTAL*100,
         PC_BACHELORS = BACHELORS/POPOVER25*100,
         PC_GRADPROF = GRADPROF/POPOVER25*100,
         RENT_TENURE = RENTEROCC/(RENTEROCC + OWNEROCC)*100,
         PC_LEP = LEP/POPTOTAL*100,
         PC_HH_MOBILE = HHMOBILE/HHTOTAL*100,
         PC_HH_NOVEH = HH_NOVEH/HHTOTAL*100,
         PC_BLACK = BLACKNH/POPTOTAL*100,
         PC_ASIAN = ASIANNH/POPTOTAL*100,
         PC_NATIVE = AMINDNH/POPTOTAL*100,
         PC_TWORACE = MULTRACENH/POPTOTAL*100,
         PC_HAWAIIANPCFIS = PACIFICNH/POPTOTAL*100,
         PC_OTHERRACE = MULTRACENH/POPTOTAL*100,
         PC_WHITE = WHITENH/POPTOTAL*100,
         PC_HISPPOP = HISPPOP/POPTOTAL*100,
         PC_POV185 = POV185RATE/POPTOTAL*100) %>%
  dplyr::select(GEOID2, PC_POC, PC_18UNDER, PC_65UP, PC_BACHELORS, RENT_TENURE, PC_LEP, PC_HH_MOBILE, PC_HH_NOVEH, PC_GRADPROF, PC_BLACK, PC_ASIAN, PC_NATIVE, PC_TWORACE, PC_WHITE, PC_HISPPOP, PC_HAWAIIANPCFIS, PC_OTHERRACE, PC_POV185, MEDGRENT, MEDHOMEVAL, MEDIANHHI)

acs_yr_tidy <<- acs %>%
  gather(2:length(acs), key = "variable", value = "value") %>%
  spread(variable, value = value)

colnames(acs_yr_tidy) <<- paste(colnames(acs_yr_tidy), year, sep = "_")

assign(paste("acs", year, sep = "_"), acs_yr_tidy, envir = .GlobalEnv)

}

df_list <- list(acs2010, acs2017)

# Run cleaning function over ACS files
map(df_list, acs_df)

# 2010, 2017 datasets to join to others: acs_2010 (Tract), acs_2017 (Tract)
```

## Clean 2000 data

```{r}

# Read in variables for 2000 block groups - tract = first 11 digits of the block group ID
census_00 <- read_csv("Data/CEN2000_2010boundaries2.csv")

census_00_rename <- census_00 %>%
  rename(MEDHOMEVAL = HHMEDVAL,
         MEDGRENT = GROSSRENT,
         MEDIANHHI = HHMEDINC)

census_00_tr <- census_00_rename %>%
  dplyr::select(noquote(order(colnames(census_00_rename)))) %>%
  gather(11:26, key = "Pc_cat", value = "Value_pc") %>%
  mutate(Value_gross = ifelse(Pc_cat == "PC_BACHELORS" | Pc_cat == "PC_GRADPROF", Value_pc * POPOVER25, Value_pc * TOTALPOP),
         Renters = RENT_TENURE * OCC_HOUSING) %>%
  dplyr::select(-Value_pc) %>%
  spread(Pc_cat, value = Value_gross) %>%
  mutate(AA_TR10 = substring(GEOID10, 1, 11)) %>%
  dplyr::select(-AVGHHSIZE, -MEDGRENT, -MEDHOMEVAL, -MEDIANHHI, -RENT_TENURE) # remove categories that aren't summed

# 2000 variables to be summed - summed
tracts_00_sum <- census_00_tr %>%
  dplyr::select(noquote(order(colnames(census_00_tr)))) %>%
  dplyr::select(-GEOID10) %>%
  gather(2:25, key = "Category", value = "Value") %>%
  group_by(AA_TR10, Category) %>%
  mutate(Value_summed = sum(Value, na.rm = T)) %>%
  ungroup() %>%
  dplyr::select(-Value) %>%
  unique() %>%
  spread(Category, value = Value_summed) %>%
  dplyr::select(AA_TR10, OCC_HOUSING, TOTALPOP, POPOVER25, HOUSEHOLDS, everything()) %>%
  gather(6:25, key = "Category", value = "Value") %>%
  mutate(Category_prefix = substring(Category, 1, 2)) %>%
  mutate(Percentage_value = ifelse(Category_prefix == "PC" & Category != "PC_BACHELORS" & Category != "PC_GRADPROF", Value/TOTALPOP * 100,
                                   ifelse(Category == "HISPOP", Value/TOTALPOP * 100,
                                          ifelse(Category == "PC_BACHELORS" | Category == "PC_GRADPROF", Value/POPOVER25 *100,
                                                 ifelse(Category == "Renters", Value/OCC_HOUSING * 100,
                                                        ifelse(Category == "HHMOBILE", Value/HOUSEHOLDS *100, Value)))))) %>%
  dplyr::select(AA_TR10, Percentage_value, TOTALPOP, Category) %>%
  rename(TR10 = AA_TR10) %>%
  mutate(Year = 2000) %>%
  unite(Category_yr, Category, Year) %>%
  spread(Category_yr, value = Percentage_value) %>%
  rename(Tract = TR10,
         POPTOTAL_EST_2000 = TOTALPOP,
         HUTOTAL_EST_2000 = HOUSING_UNITS_2000,
         RENT_TENURE_2000 = Renters_2000,
         PC_HISPPOP_2000 = HISPOP_2000,
         PC_HH_NOVEH_2000 = PC_HHNOVEH_2000,
         PC_LEP_SPAN_2000 = PC_LEPSPAN_2000,
         PC_HH_MOBILE_2000 = HHMOBILE_2000) %>%
  dplyr::select(-PC_LEP_SPAN_2000)

# 2000 medians and averages - from Dennis' dataset
med_av_00 <- read_csv("Data/cleaned_variables2000.csv")

# Select median gross rent, median EMV and median HHI from 2000 dataset
rent_emv_hhi_00 <- med_av_00 %>%
  dplyr::select(medHHinc2000, MedianValue2000, rent2000, TRACT) %>%
  rename(MEDGRENT_2000 = rent2000,
         MEDHOMEVAL_2000 = MedianValue2000,
         MEDIANHHI_2000 = medHHinc2000,
         Tract = TRACT) %>%
  mutate(Tract = as.character(Tract))

# 2000 datasets to be joined to others: rent_emv_hhi_00 (Tract), tracts_00_sum (Tract)
```

# Small area population and housing-unit estimates

```{r}
# Get Matt's estimates of population and housing units
sa <- read.xlsx("Data/estimates_V2018_tract.xlsx") 

sa17 <- sa %>%
  filter(EST_YEAR == 2017) %>%
  dplyr::select(TR10, EST_YEAR, POPTOTAL_EST, HUTOTAL_EST) %>%
  rename(Tract = TR10) %>%
  mutate(HU_NAME = "HUTOTAL_EST",
         POP_NAME = "POPTOTAL_EST",
         EST_YEAR2 = EST_YEAR) %>%
  unite(POP_YR, POP_NAME, EST_YEAR) %>%
  unite(HU_YR, HU_NAME, EST_YEAR2) %>%
  spread(POP_YR, value = POPTOTAL_EST) %>%
  spread(HU_YR, value = HUTOTAL_EST)

sa10 <- sa %>%
  dplyr::select(TR10, POPTOTAL10, HUTOTAL10) %>%
  unique() %>%
  rename(POPTOTAL_EST_2010 = POPTOTAL10,
         HUTOTAL_EST_2010 = HUTOTAL10,
         Tract = TR10)

# SA datasets to be joined to others: sa17 (Tract), sa10 (Tract)

```

## % new builds

```{r}
#gunzip('Data/acs20175_all.csv.gz', remove = F) # Unzip compressed gz file, don't remove compressed file
acs_all <- fread("Data/Tract-level/acs20175_all.csv")

# New builds
new_builds <- acs_all %>%
  dplyr::select(B25034e1, B25034e2, B25034e3, B25034e4, B25034e5, SUMLEV, TCFLAG, GEOID, GEOID2, GEONAME) %>%
  filter(SUMLEV == 140 & TCFLAG == 1) %>% # Summary level of geography = tract & contained within Twin Cities
  mutate(Total_HU_2017 = B25034e1, # Total housing units in existence in 2017
         HU_2010_2017 = B25034e3 + B25034e2,  # Units built 2010 to 2013, units built 2014 or later
         HU_2000_2009 = B25034e4,
         HU_1990_1999 = B25034e5) %>%
   mutate(PC_NEW_HU_2000 = HU_1990_1999/(Total_HU_2017-HU_2010_2017-HU_2000_2009)*100,
          PC_NEW_HU_2010 = HU_2000_2009/(Total_HU_2017-HU_2010_2017)*100,
          PC_NEW_HU_2017 = HU_2010_2017/Total_HU_2017*100) %>%
   dplyr::select(GEOID2,PC_NEW_HU_2000, PC_NEW_HU_2010, PC_NEW_HU_2017) %>%
  rename(Tract = GEOID2)

# fwrite(new_builds, "Data/Tract-level/new_builds.csv")
# New build dataset to be joined:  new_builds (Tract)
```

# Join all data

```{r}
acs_00 <- full_join(tracts_00_sum, rent_emv_hhi_00, by = "Tract") # Join 2000 datasets together
acs_0010 <- full_join(acs_00, acs_2010, by = c("Tract" = "GEOID2_2010")) # Add 2010
acs_0017 <- full_join(acs_0010, acs_2017, by = c("Tract" = "GEOID2_2017")) # Add 2017
acs_sa10 <- full_join(acs_0017, sa10, by = "Tract") # Add SA estimates for 2010
acs_sa <- full_join(acs_sa10, sa17, by = "Tract") # Add SA estimates for 2017
acs_all_vars <- full_join(acs_sa, new_builds, by = "Tract") %>%
  rename(AA_TRACT = Tract) %>% # rename tract so it's first in columns
  unique() #For some reason, four tracts get joined twice, but have same values for all variables...? Remove duplicates

cluster_num <- acs_all_vars %>%
  gather(2:length(acs_all_vars), key = "var_name", value = "var_value") %>%
  mutate(var_value = as.numeric(var_value)) %>%
  spread(var_name, value = var_value)

cluster_data <- cluster_num %>%
  dplyr::select(noquote(order(colnames(cluster_num)))) %>%
  rename(Tract = AA_TRACT)

```

# Random forest predictions for HHI, EMV, rent

```{r}
# cluster_imp <- cluster_data %>%
#   mutate(Tract = as.numeric(Tract)) %>% # Keep Tract number as numeric as a proxy for spatial relationships
#   as.data.frame()
# 
# # Impute missing values, using all parameters as default values
# missing_imputed <- missForest(cluster_imp)
# 
# cluster_full <- as_tibble(missing_imputed$ximp)
# imputation_OOB_error <- missing_imputed$OOBerror
# 
# fwrite(cluster_full, "Data/3-Timepoint Random Forest Imputation Dataset (with geo proxy).csv")

# Impute missing values, using all params as default values & excluding Tract as a predictor
imp_sans_geo <- cluster_data %>%
  dplyr::select(-Tract) %>% # Drop Tract
  as.data.frame()

imputed_sans_geo <- missForest(imp_sans_geo)

cluster_full_ngeo <- as_tibble(imputed_sans_geo$ximp)
imputation_ngeo_OOB_error <- imputed_sans_geo$OOBerror

cluster_full_ngeo$Tract <- cluster_data$Tract

fwrite(cluster_full_ngeo, "Data/3-Timepoint Random Forest Imputation Dataset (sans geo proxy).csv")

ggplot(cluster_full, aes(MEDIANHHI_2017, MEDGRENT_2017)) +
  geom_point() +
  geom_smooth(method = lm)

#summary(lm(MEDGRENT_2000 ~ MEDIANHHI_2000, data = cluster_full))

# Exclude non-MUSA tracts
nonMUSA <- read_csv("Data/SNC Tracts Incl Cities.csv") %>% mutate(Tract = as.character(GEOID))

cluster_snc <- inner_join(cluster_full_ngeo, nonMUSA, by = "Tract") %>%
  mutate(Tract = as.character(Tract)) %>%
  dplyr::select(Tract, everything(), -GEOID) %>%
  unique() %>%
  as.data.frame()

# fwrite(cluster_snc, "Data/3-Timepoint Random Forest Imputation Sans Geo, non-MUSA.csv")

```

# Add in EMV from parcel data; drop ACS home value data

```{r}
cluster_snc <- fread("Data/3-Timepoint Random Forest Imputation Sans Geo, non-MUSA.csv")
parcel_emv <- fread("Data/Tract-level/emv_00_10_17.csv")

# Adjust parcel EMVs to 2017 dollars with Case Shiller Index
emv_adj <- parcel_emv %>%
  mutate(EMV_2000 = EMV_2000*(156.52657/100),
         EMV_2010 = EMV_2010*(156.52657/123.85143))

# Adjust HHI and rent to 2017 dollars with Consumer Product Index
cluster_full <- left_join(cluster_snc, emv_adj, by = "Tract") %>%
  dplyr::select(-MEDHOMEVAL_2000, -MEDHOMEVAL_2010, -MEDHOMEVAL_2017, -PC_HAWAIIANPCFIS_2000, -PC_HAWAIIANPCFIS_2010, -PC_HAWAIIANPCFIS_2017, -PC_NATIVE_2000, -PC_NATIVE_2010, -PC_NATIVE_2017) %>%
  unique() %>%
  mutate(MEDGRENT_2000 = MEDGRENT_2000*(229.874/168.3),
         MEDGRENT_2010 = MEDGRENT_2010*(229.874/208.046),
         MEDIANHHI_2000 = MEDIANHHI_2000*(229.874/168.3),
         MEDIANHHI_2010 = MEDIANHHI_2010*(229.874/208.046)) %>%
  mutate(Tract = as.character(Tract))


# Impute missing EMV values, using all params as default values & excluding Tract as a predictor
cluster_sans_tract <- cluster_full %>%
  dplyr::select(-Tract) %>% # Drop Tract
  as.data.frame()

emv_imputed <- missForest(cluster_sans_tract)

clusters_imputed <- as_tibble(emv_imputed$ximp)
imputation_OOB_error <- emv_imputed$OOBerror

clusters_imputed$Tract <- cluster_full$Tract

# fwrite(clusters_imputed, "Data/3-Timepoint RF-Imputed Parcel EMV Dataset.csv")

```

# Get order of variables and assignment for clustering algorithm

```{r}
cluster_full <- fread("Data/3-Timepoint RF-Imputed Parcel EMV Dataset.csv")

cluster_data <- cluster_full %>%
  mutate(Tract = as.factor(Tract)) %>%
  as.data.frame() %>%
  dplyr::select(-PC_HH_MOBILE_2000, -PC_HH_MOBILE_2010, -PC_HH_MOBILE_2017) %>% # Remove hh mobile (no interaction)
  dplyr::select(Tract, everything())

# Get order for clustering
assign <- cluster_data %>%
  gather(everything(), key = "Variable_name", value = "var_value") %>%
  dplyr::select(Variable_name) %>%
  unique() %>%
  mutate(ID = row_number()) %>%
  filter(Variable_name != "Tract") %>%
  separate(Variable_name, into = c("Variable", "Year"), sep = -4) %>%
  mutate(Year = as.numeric(Year)) %>%
  group_by(Variable) %>%
  mutate(Year_min = min(Year),
         Year_max = max(Year)) %>%
  ungroup() %>%
  filter(Year == Year_min | Year == Year_max) %>%
  dplyr::select(Variable, ID) %>%
  group_by(Variable) %>%
  summarize(Assignments = paste(ID, collapse = "_")) %>%
  separate(Assignments, into = c("Min", "Max"), sep = "_") %>%
  mutate(Min = as.numeric(Min),
         Max = as.numeric(Max)) %>%
  ungroup() %>%
  filter(Variable != "PC_WHITE_") # Don't include % white in clustering

# Create list of cluster variables for clustering
time_in_data <- vector("list", nrow(assign))
for_length <- 1:nrow(assign)

for (i in for_length) {
  
  assign_slice <- assign %>% slice(i)
  time_in_data[[i]] <- assign_slice$Min:assign_slice$Max

}

names(time_in_data) <- assign$Variable

```

# Longitudinal clustering on non-MUSA

```{r}
# Input to cluster must be a data.frame only (not a tibble as well) & cannot contain duplicates - make sure all observations are unique with unique IDs; ID (in this case Tract #) must be a character, NOT integer64 (from data.table package); ID # MUST come first (first column) of data
cld_snc_3 <- cld3d(cluster_data, timeInData = time_in_data)

set.seed(410)
kml3d(cld_snc_3, nbClusters = 7) # Any number of clusters exceeding 6 must be specified using nbClusters
#choice(cld_snc)

# Take a look at 8 clusters
set.seed(410)
cluster_full$clusters <- getClusters(cld_snc_3, 7)

cluster_3_snc <- cluster_full %>%
  filter(!is.na(clusters)) %>%
  dplyr::select(Tract, clusters, everything()) %>%
  gather(3:length(cluster_full), key = "Variable_year", value = "Value") %>%
  mutate(Value = as.numeric(Value)) %>%
  separate(Variable_year, into = c("Variable", "Year"), sep = -4) %>%
  separate(Variable, into = c("Variable", "underscore"), sep = -1) %>%
  dplyr::select(-underscore)

fwrite(cluster_3_snc, "Results/Longitudinal Clustering/7-Clusters 3-Timepoints Inflation-Adjusted no hh_mobile.csv")
```

## Reverse-Engineering Clusters:  Decision Trees

```{r}

# No pruning
tree_wide_data <- cluster_snc %>%
  dplyr::select(-Tract)

# Default params, not pruning
tree_wide <- ctree(clusters ~ ., data = tree_wide_data)

png("Results/Longitudinal Clustering/Decision Tree Wide No Pruning.png", width = 6000, height = 2000)
plot(tree_wide)

# misclassification/error rate
# check whether predicted values match actual values, calculate the
# mean of that boolean vector, then subtract from 1 to get the error
# rate (original value identifies accuracy rate)
1 - mean(predict(tree_wide) == cluster_snc$clusters, na.rm = T)

```

```{r}
# Pruning with minbucket
tree_wide_data <- cluster_snc %>%
  dplyr::select(-Tract)

# Default params, not pruning
tree_wide <- ctree(clusters ~ ., data = tree_wide_data, control = ctree_control(minbucket = 50))

png("Results/Longitudinal Clustering/Decision Tree Wide Pruning minbucket.png", width = 6000, height = 2000)
plot(tree_wide)

# misclassification/error rate
# check whether predicted values match actual values, calculate the
# mean of that boolean vector, then subtract from 1 to get the error
# rate (original value identifies accuracy rate)
1 - mean(predict(tree_wide) == cluster_snc$clusters, na.rm = T) # lower accuracy than without pruning, but lower number of terminal nodes

```

```{r}
# Pruning with minsplit
tree_wide_data <- cluster_snc %>%
  dplyr::select(-Tract)

# Default params, not pruning
tree_wide <- ctree(clusters ~ ., data = tree_wide_data, control = ctree_control(nmax = c(yx = 8, z = Inf),
                                                                                minbucket = 50))

png("Results/Longitudinal Clustering/Decision Tree Wide Pruning nmax and minbucket.png", width = 6000, height = 2000)
plot(tree_wide)

# misclassification/error rate
# check whether predicted values match actual values, calculate the
# mean of that boolean vector, then subtract from 1 to get the error
# rate (original value identifies accuracy rate)
1 - mean(predict(tree_wide) == cluster_snc$clusters, na.rm = T)

```

